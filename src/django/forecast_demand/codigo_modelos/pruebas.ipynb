{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_json = {\n",
    "    \"prosumer_ID\": \"string8\",\n",
    "    \"prosumer_name\": \"string3\",\n",
    "    \"prosumer_class\": \"class2\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"01-10-2023\",\n",
    "            \"energy_demand\": [10, 12, 15, 20, 18, 22, 14, 11, 17, 23, 19, 13, 21, 16, 24, 9, 6, 8, 7, 5, 3, 4, 2, 1]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"02-10-2023\",\n",
    "            \"energy_demand\": [9, 8, 7, 12, 13, 11, 14, 6, 19, 10, 21, 20, 15, 17, 22, 5, 3, 4, 18, 23, 2, 16, 24, 1]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"03-10-2023\",\n",
    "            \"energy_demand\": [2, 8, 3, 4, 6, 1, 19, 15, 12, 14, 13, 10, 11, 9, 16, 17, 20, 7, 5, 22, 18, 23, 24, 21]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"04-10-2023\",\n",
    "            \"energy_demand\": [8, 15, 17, 4, 13, 10, 7, 2, 3, 11, 12, 9, 14, 6, 16, 5, 20, 24, 22, 23, 19, 18, 21, 1]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"05-10-2023\",\n",
    "            \"energy_demand\": [2, 4, 6, 7, 8, 9, 1, 10, 11, 12, 3, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 5]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"06-10-2023\",\n",
    "            \"energy_demand\": [15, 22, 17, 10, 7, 13, 8, 2, 6, 18, 9, 24, 3, 16, 11, 5, 19, 4, 1, 20, 14, 21, 12, 23]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"17-10-2023\",\n",
    "            \"energy_demand\": [5, 8, 3, 13, 10, 11, 7, 6, 16, 15, 4, 9, 2, 18, 12, 17, 20, 19, 1, 21, 14, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"18-10-2023\",\n",
    "            \"energy_demand\": [21, 6, 17, 24, 10, 11, 4, 2, 8, 9, 7, 5, 12, 3, 18, 13, 16, 14, 20, 19, 15, 1, 23, 22]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"19-10-2023\",\n",
    "            \"energy_demand\": [2, 8, 12, 9, 6, 15, 13, 1, 11, 10, 7, 4, 3, 5, 18, 17, 20, 14, 16, 19, 22, 21, 24, 23]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"20-10-2023\",\n",
    "            \"energy_demand\": [24, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"21-10-2023\",\n",
    "            \"energy_demand\": [14, 19, 8, 20, 10, 12, 15, 21, 6, 7, 4, 11, 16, 13, 18, 2, 17, 23, 1, 22, 24, 9, 3, 5]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"22-10-2023\",\n",
    "            \"energy_demand\": [3, 9, 10, 16, 14, 12, 11, 4, 8, 13, 7, 5, 15, 18, 2, 1, 6, 17, 19, 20, 21, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"23-10-2023\",\n",
    "            \"energy_demand\": [7, 1, 4, 15, 2, 3, 5, 8, 6, 12, 9, 13, 11, 10, 14, 16, 18, 17, 20, 19, 21, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": False,\n",
    "            \"date\": \"24-10-2023\",\n",
    "            \"energy_demand\": [12, 20, 6, 7, 13, 5, 8, 15, 16, 10, 9, 18, 11, 2, 3, 4, 1, 17, 14, 19, 21, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"25-10-2023\",\n",
    "            \"energy_demand\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "        },\n",
    "                {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"26-10-2023\",\n",
    "            \"energy_demand\": [7, 1, 4, 15, 2, 3, 5, 8, 6, 12, 9, 13, 11, 10, 14, 16, 18, 17, 20, 19, 21, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": False,\n",
    "            \"date\": \"27-10-2023\",\n",
    "            \"energy_demand\": [12, 20, 6, 7, 13, 5, 8, 15, 16, 10, 9, 18, 11, 2, 3, 4, 1, 17, 14, 19, 21, 22, 23, 24]\n",
    "        },\n",
    "        {\n",
    "            \"is_workday\": True,\n",
    "            \"date\": \"28-10-2023\",\n",
    "            \"energy_demand\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def process_data(df, localizacion, use_predictions, workday_df):\n",
    "    # solicitar_historico_consumo\n",
    "\n",
    "    if use_predictions:\n",
    "        df_aemet = pd.read_csv(f'./predictions/{localizacion}.csv',  parse_dates=['datetime'], index_col=['datetime'])\n",
    "        df_aemet.index.rename('Date', inplace=True)\n",
    "        # TO DO  se podrian incluir  'sky_00-06', 'sky_06-12', 'sky_12-18', 'sky_18-24' pero al ser categoricas no es trivial realizar el one_hot_encodig de la misma forma en entrenamiento y en inferencia\n",
    "        df_aemet = df_aemet[['rain_00-06', 'rain_06-12', 'rain_12-18', 'rain_18-24', 'wind_00-06', 'wind_06-12', 'wind_12-18', 'wind_18-24', 'temp_6', 'temp_12', 'temp_18', 'temp_24', 'hum_6', 'hum_12', 'hum_18', 'hum_24']]\n",
    "        \n",
    "\n",
    "    # Cargar datos historicos temperaturas en un DF, cambiar sus formatos a float y indice por fecha\n",
    "\n",
    "\n",
    "    # Filtrar features historicos con mas de un 5% de NaN  (Guardar nombres de las columnas para inferencia?)\n",
    "    nans = df.isna().sum()\n",
    "    nans = nans / len(df) * 100\n",
    "    nans = nans.sort_values(ascending=True)\n",
    "    features = nans[nans<5].index.tolist()\n",
    "    df = df[features]\n",
    "    print(f\"Features filtradas {features}\")\n",
    "    df.dropna(subset=[\"Value\"], inplace=True)  # Elimini filas con Potencia total vacías\n",
    "\n",
    "    # Filtramos outliers\n",
    "    print(\"Old Shape: \", df.shape)\n",
    "    Q1 = np.percentile(df['Value'], 25)\n",
    "    Q3 = np.percentile(df['Value'], 75)\n",
    "    print(f'Q1:{Q1},  Q2:{Q3}')\n",
    "    IQR = Q3 - Q1\n",
    "    upper=Q3+1.5*IQR\n",
    "    mask = df['Value']<=upper\n",
    "    df = df[mask]\n",
    "    print(\"Upper Bound:\",upper)\n",
    "    #Below Lower bound\n",
    "    lower=Q1-1.5*IQR\n",
    "    mask = df['Value']>=lower\n",
    "    df = df[mask]\n",
    "    print(\"lower Bound\",lower)\n",
    "    print(\"New Shape: \", df.shape)\n",
    "\n",
    "\n",
    "    # Eliminar dias sin todas las horas\n",
    "    df['Date'] = df['Date'].dt.floor('H')\n",
    "    df = df.groupby('Date', as_index=False).mean()\n",
    "    # elimino dias con horas sin muestras\n",
    "    df['day'] = df['Date'].dt.floor('D')\n",
    "\n",
    "    df = df.groupby('day', as_index=False).apply(lambda x: x if len(x)==24 else None)\n",
    "    df = df.drop(['day'], axis=1)\n",
    "    index = pd.DatetimeIndex(df['Date'])\n",
    "    df = df.drop(['Date'], axis=1)\n",
    "\n",
    "    print(\"escalado\")\n",
    "    # Escalado de datos   y   Guardo desviacion tipica y media de los datos de consumo total para desescalar las predicciones\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    df = scaler.transform(df)\n",
    "\n",
    "    # Guardo desviacion tipica y media de los datos de consumo total para desescalar las predicciones\n",
    "    desviacion_tipica = np.sqrt(scaler.var_)[0]\n",
    "    media = scaler.mean_[0]\n",
    "    print(desviacion_tipica, media)\n",
    "    print(f'desviación típica: {desviacion_tipica:.2f}, media: {media:.2f}')\n",
    "\n",
    "    #  Dataframe poner indice de nuevo tras escalar\n",
    "    df = pd.DataFrame(df, columns=features[1:])\n",
    "    df.index = index  \n",
    "\n",
    "\n",
    "    # --- AÑADIMOS FEATURES ---\n",
    "    #   Extraemos Features ES_LABORABLE  y la añadimos al dataframe  \n",
    "\n",
    "    #   SIN COS de Dia de la Semana\n",
    "    df['dia_sin'] = df.index.map(lambda x: np.sin(x.weekday() * (2.*np.pi/6)))\n",
    "    df['dia_cos'] = df.index.map(lambda x: np.cos(x.weekday() * (2.*np.pi/6)))\n",
    "\n",
    "    #   SIN COS de Mes\n",
    "    df['mes_sin'] = df.index.map(lambda x: np.sin(x.month - 1 * (2.*np.pi/12)))\n",
    "    df['mes_cos'] = df.index.map(lambda x: np.cos(x.month - 1 * (2.*np.pi/12)))\n",
    "\n",
    "    #   Variables dia anterior\n",
    "    arr = np.array(df['Value'])\n",
    "    arr_media = []\n",
    "    arr_min = []\n",
    "    arr_max = []\n",
    "    for i in range(int(len(arr)/24)):\n",
    "        for a in range(24):\n",
    "            arr_media.append(np.mean(arr[i*24:(i+1)*24]))\n",
    "            arr_min.append(np.min(arr[i*24:(i+1)*24]))\n",
    "            arr_max.append(np.max(arr[i*24:(i+1)*24]))\n",
    "        \n",
    "        # AÑADO 24 0s YA QUE DEBEN ESTAR EN LAS FILAS DEL DIA SIGUIENTE\n",
    "        # Aunque el dia anterior no siempre es ayer ya que faltan dias en el dataset, no pasa nada ya que al no existir el anterior, no hay X y no se creará ese dia como target.\n",
    "    df['max_dia_ant'] = np.concatenate([np.zeros(24), np.array(arr_min)[:-24]])\n",
    "    df['min_dia_ant'] = np.concatenate([np.zeros(24), np.array(arr_max)[:-24]])\n",
    "    df['media_dia_ant'] = np.concatenate([np.zeros(24), np.array(arr_media)[:-24]])\n",
    "\n",
    "    # is_workday\n",
    "    df = pd.merge(df, workday_df, on='Date', how='left')\n",
    "\n",
    "\n",
    "    if use_predictions:\n",
    "        #   Temperaturas\n",
    "        # separo las fechas\n",
    "        index = df_aemet.index\n",
    "        features = list(df_aemet.columns)\n",
    "        # escalo los datos\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_aemet)\n",
    "        df_aemet = scaler.transform(df_aemet)\n",
    "        # Dataframe con indice de vuelta\n",
    "        df_aemet = pd.DataFrame(df_aemet, columns=features)\n",
    "        df_aemet.index = index\n",
    "        # Calculamos los datetimes de todas las horas que pertenecen a los dias con predicciones\n",
    "        all_dates = []\n",
    "        for date_obj in df_aemet.index:\n",
    "            hours_of_day = []\n",
    "            # Create a datetime object with the same date and iterate through hours\n",
    "            for hour in range(24):\n",
    "                hour_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, 0)\n",
    "                hours_of_day.append(hour_datetime)\n",
    "            all_dates += hours_of_day\n",
    "        # Eliminamos los dias para los cuales no hay datos históricos de predicciones\n",
    "        df = df[df['Date'].isin(all_dates)]\n",
    "        # Agregamos los datos del clima a cada primera hora del dia 00:00, las demas horas del dia quedan rellenas de NaN\n",
    "        df = df.join(df_aemet, on='Date')\n",
    "        display(df)\n",
    "        \n",
    "    # Elimino nans de forma temporal de esta forma \n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    #   Establecer variable (constante)  \"variables_condicionales\"\n",
    "    INPUT_DAYS = 1\n",
    "    variables_condicionales = 8   #  2 X SIN COS de Dia de la Semana     2 X SIN COS de Mes     3 X  Variables dia anterior     1 X Workday\n",
    "\n",
    "    if use_predictions:\n",
    "        variables_condicionales += df_aemet.shape[1]\n",
    "\n",
    "\n",
    "    df = df.set_index('Date')\n",
    "    display(df)\n",
    "\n",
    "    #   Creamos ventanas y obtenemos   X_seq, X_condition, y, Date\n",
    "    X_seq, X_condition, y  = crear_ventanas(df, INPUT_DAYS, variables_condicionales)\n",
    "\n",
    "    #   Dividimos nuestro dataset en solo en Train y Validacion. Creo tupla de val\n",
    "    train_val_seq, train_val_cond, train_val_y = X_seq[:int(0.8*len(X_seq))], X_condition[:int(0.8*len(X_condition))], y[:int(0.8*len(y))]\n",
    "\n",
    "    test = (X_seq[int(0.8*len(X_seq)):], X_condition[int(0.8*len(X_condition)):], y[int(0.8*len(y)):])\n",
    "\n",
    "    # Shufle train y saco validacion de este conjunto\n",
    "    permutation = np.random.permutation(len(train_val_seq))\n",
    "    #print(f\"permutatioon: {permutation}\")\n",
    "    train_val_seq, train_val_cond, train_val_y = train_val_seq[permutation], train_val_cond[permutation], train_val_y[permutation]\n",
    "\n",
    "    # Divido train y val\n",
    "    train_seq, train_cond, train_y = train_val_seq[:int(0.8 * len(train_val_seq))], train_val_cond[:int(0.8 * len(train_val_cond))], train_val_y[:int(0.8 * len(train_val_y))]\n",
    "    val_seq, val_cond, val_y = train_val_seq[int(0.8 * len(train_val_seq)):], train_val_cond[int(0.8 * len(train_val_cond)):], train_val_y[int(0.8 * len(train_val_y)):]\n",
    "    #print(f\"train_seq: {train_seq.shape}\")\n",
    "    #print(f\"val_seq: {val_seq.shape}\")\n",
    "    # Creo tuplas de train y val\n",
    "    train = (train_seq, train_cond, train_y)\n",
    "    val = (val_seq, val_cond, val_y)\n",
    "\n",
    "\n",
    "    #   Se crea tf.Dataset\n",
    "    train_dataset, val_dataset, test_dataset  = get_dataset(train, val, test)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def crear_ventanas(df, INPUT_DAYS, variables_condicionales):\n",
    "    print(f\"variables_condicionales {variables_condicionales}\")\n",
    "    dates = df.index\n",
    "    X_seq, X_condition, y = [], [], []\n",
    "\n",
    "    \n",
    "    for date in dates.normalize().unique():\n",
    "        input_start = date\n",
    "        input_end = date + pd.Timedelta(days=INPUT_DAYS-1, hours=23)  # calculamos la fecha de fin de la secuencia\n",
    "        target_start = date + pd.Timedelta(days=INPUT_DAYS)\n",
    "        target_end = target_start + pd.Timedelta(hours=23)\n",
    "        \n",
    "\n",
    "        if all(pd.date_range(input_start, target_end, freq='H').isin(dates)):  # verificamos si existe una secuencia completa de longitud len_seq\n",
    "            input_seq = pd.date_range(input_start, input_end, freq='H')\n",
    "            target_seq = pd.date_range(target_start, target_end, freq='H')\n",
    "            \n",
    "            \"\"\"\n",
    "            print(\"---------------\")\n",
    "            print(input_start, input_end)\n",
    "            print(target_start, target_end)\n",
    "            print(input_seq)\n",
    "            print(target_seq)\n",
    "            \"\"\"\n",
    "            # Si solo hemos cogido la variable consumo total del DF, ajustamos las dimensiones para la LSTM\n",
    "            if str(type(df)) == \"<class 'pandas.core.series.Series'>\":\n",
    "                y.append(np.expand_dims(df.loc[target_seq].to_numpy().astype('float32'), axis=-1))\n",
    "                X_seq.append(np.expand_dims(df.loc[input_seq].to_numpy().astype('float32'), axis=-1))\n",
    "                \n",
    "            # En caso contrario solamente creamos las secuencias\n",
    "            else:\n",
    "                y.append(df.loc[target_seq][\"Value\"].to_numpy().astype('float32'))\n",
    "                X_seq.append(df.loc[input_seq].to_numpy().astype('float32')[:,:-variables_condicionales])\n",
    "                X_condition.append(df.loc[target_seq].iloc[0, -variables_condicionales:].to_numpy().astype('float32'))\n",
    "                \"\"\"\n",
    "                print(input_seq)\n",
    "                print(target_seq)\n",
    "                display(X_seq[-1])\n",
    "                display(X_condition[-1])\n",
    "                display(y[-1])\n",
    "                print('-------------')\n",
    "                \"\"\"\n",
    "                \n",
    "    return np.array(X_seq), np.array(X_condition), np.array(y)\n",
    "\n",
    "\n",
    "def get_dataset(train_fold, val_fold, test_fold):\n",
    "    X_seq_train, X_condition_train, y_train = train_fold\n",
    "    X_seq_val, X_condition_val, y_val = val_fold\n",
    "    X_seq_test, X_condition_test, y_test = test_fold\n",
    "    \n",
    "    train_input = tf.data.Dataset.from_tensor_slices((X_seq_train, X_condition_train))\n",
    "    train_target =  tf.data.Dataset.from_tensor_slices(y_train)\n",
    "    train =  tf.data.Dataset.zip((train_input, train_target))\n",
    "\n",
    "    val_input = tf.data.Dataset.from_tensor_slices((X_seq_val, X_condition_val))\n",
    "    val_target = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "    val = tf.data.Dataset.zip((val_input, val_target))\n",
    "\n",
    "    test_input = tf.data.Dataset.from_tensor_slices((X_seq_test, X_condition_test))\n",
    "    test_target = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "    test = tf.data.Dataset.zip((test_input, test_target))\n",
    "\n",
    "    train = train.batch(64).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "    train.shuffle(1000)\n",
    "    val = val.batch(64).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "    val.shuffle(1000)\n",
    "    test = test.batch(64).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "\n",
    "    #print(y_train.shape)\n",
    "    print(f'Train shapes:   Condition:{X_condition_train[0].shape}  Seq:{X_seq_train[0].shape}  target:{y_train[0].shape}')\n",
    "    print(f'Val shapes:   Condition:{X_condition_val[0].shape}  Seq:{X_seq_val[0].shape}  target:{y_val[0].shape}')\n",
    "    print(f'test shapes:   Condition:{X_condition_test[0].shape}  Seq:{X_seq_test[0].shape}  target:{y_test[0].shape}')\n",
    "   \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def get_df(data):\n",
    "    df = []\n",
    "    workday_df = {'Date':[], 'Workday':[]}\n",
    "\n",
    "    for el in data:\n",
    "        try:\n",
    "            date = el['date']\n",
    "            date_obj = datetime.strptime(date, \"%d-%m-%Y\")\n",
    "            # Initialize the list to store all hours\n",
    "            hours_of_day = []\n",
    "\n",
    "            # Create a datetime object with the same date and iterate through hours\n",
    "            for hour in range(24):\n",
    "                hour_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, 0)\n",
    "                hours_of_day.append(hour_datetime)\n",
    "\n",
    "            energy = el['energy_demand']\n",
    "            # Create energy demand array\n",
    "            if len(energy) != 24:\n",
    "                continue\n",
    "            \n",
    "            # Create disct with working days\n",
    "            workday_df['Date'] += [date_obj]\n",
    "            workday_df['Workday'] += [1] if el['is_workday'] else [0]\n",
    "\n",
    "\n",
    "            arr = np.column_stack([hours_of_day, energy])\n",
    "            df.append(arr)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "\n",
    "    return pd.DataFrame(np.concatenate(df, axis=0), columns=['Date', 'Value']), pd.DataFrame.from_dict(workday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './predictions/01019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\inaki.campo\\Desktop\\API brain\\sw-backend\\django\\forecast_demand\\codigo_modelos\\pruebas.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df, workday_df \u001b[39m=\u001b[39m get_df(data_json[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_dataset, val_dataset, test_dataset \u001b[39m=\u001b[39m process_data(df, \u001b[39m'\u001b[39;49m\u001b[39m01019\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, workday_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# TO DO   196  variables condicionales ?\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\inaki.campo\\Desktop\\API brain\\sw-backend\\django\\forecast_demand\\codigo_modelos\\pruebas.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_data\u001b[39m(df, localizacion, use_predictions, workday_df):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39m# solicitar_historico_consumo\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39mif\u001b[39;00m use_predictions:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m         df_aemet \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./predictions/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlocalizacion\u001b[39m}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,  parse_dates\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m'\u001b[39;49m], index_col\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdatetime\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m         df_aemet\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mrename(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/inaki.campo/Desktop/API%20brain/sw-backend/django/forecast_demand/codigo_modelos/pruebas.ipynb#W1sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         \u001b[39m# TO DO  se podrian incluir  'sky_00-06', 'sky_06-12', 'sky_12-18', 'sky_18-24' pero al ser categoricas no es trivial realizar el one_hot_encodig de la misma forma en entrenamiento y en inferencia\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\inaki.campo\\AppData\\Local\\anaconda3\\envs\\brain_predictivo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\inaki.campo\\AppData\\Local\\anaconda3\\envs\\brain_predictivo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\inaki.campo\\AppData\\Local\\anaconda3\\envs\\brain_predictivo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\inaki.campo\\AppData\\Local\\anaconda3\\envs\\brain_predictivo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\inaki.campo\\AppData\\Local\\anaconda3\\envs\\brain_predictivo\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './predictions/01019.csv'"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "df, workday_df = get_df(data_json['data'])\n",
    "df['Value'] = df['Value'].astype('float')\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = process_data(df, '01019', True, workday_df)\n",
    "\n",
    "\n",
    "\n",
    "# TO DO   196  variables condicionales ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([24, 1]), TensorShape([8]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_shape = next(iter(train_dataset))[0][0].shape[1:]\n",
    "cond_shape = next(iter(train_dataset))[0][1].shape[1:]\n",
    "input_shapes = (seq_shape, cond_shape)\n",
    "input_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_predictivo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
